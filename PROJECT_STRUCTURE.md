# Project Structure - Complete Overview

## Directory Organization

```
News-Calendar/                          # Root project directory
â”‚
â”œâ”€â”€ .github/                            # GitHub Actions workflows (KEEP FOR REFERENCE)
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ monthly-updater.yml
â”‚       â”œâ”€â”€ daily-sync.yml
â”‚       â””â”€â”€ realtime-fetcher.yml
â”‚
â”œâ”€â”€ .claude/                            # Claude Code configuration
â”‚
â”œâ”€â”€ old_structure/                      # REFERENCE ONLY - Previous implementation
â”‚   â”œâ”€â”€ FINAL_TOOLS_OUTPUT/             # All old scraper code
â”‚   â”‚   â”œâ”€â”€ scraper_core.py             # Core scraping class
â”‚   â”‚   â”œâ”€â”€ database.py                 # Database operations
â”‚   â”‚   â”œâ”€â”€ scrape_forexfactory.py      # Year-long scraper
â”‚   â”‚   â”œâ”€â”€ monthly_updater.py          # Monthly job
â”‚   â”‚   â”œâ”€â”€ daily_sync.py               # Daily job
â”‚   â”‚   â”œâ”€â”€ realtime_fetcher.py         # 5-minute job
â”‚   â”‚   â”œâ”€â”€ data_reconciliation.py      # Reconciliation logic
â”‚   â”‚   â””â”€â”€ *.py                        # Other utilities
â”‚   â”‚
â”‚   â””â”€â”€ github_workflows/               # Old GitHub Actions (for reference)
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ scraper_2.2/                        # NEW IMPLEMENTATION (Fresh start)
â”‚   â”œâ”€â”€ README.md                       # Getting started guide
â”‚   â”œâ”€â”€ requirements.txt                # Python dependencies
â”‚   â”œâ”€â”€ config.yaml                     # Configuration template
â”‚   â”œâ”€â”€ .env.example                    # Environment template
â”‚   â”‚
â”‚   â”œâ”€â”€ src/                            # Core implementation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ scraper.py                  # Main scraper
â”‚   â”‚   â”œâ”€â”€ database.py                 # DB operations
â”‚   â”‚   â”œâ”€â”€ cache.py                    # Caching layer
â”‚   â”‚   â””â”€â”€ utils.py                    # Utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ jobs/                           # Scheduled jobs
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ bulk_fetcher.py             # Bulk fetch (4 hours)
â”‚   â”‚   â”œâ”€â”€ realtime_updater.py         # Real-time (5 minutes)
â”‚   â”‚   â””â”€â”€ scheduler.py                # Job orchestration
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/                          # Unit tests
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_scraper.py
â”‚   â”‚   â”œâ”€â”€ test_cache.py
â”‚   â”‚   â””â”€â”€ test_database.py
â”‚   â”‚
â”‚   â”œâ”€â”€ logs/                           # Application logs
â”‚   â”‚   â””â”€â”€ .gitkeep
â”‚   â”‚
â”‚   â””â”€â”€ docs/
â”‚       â”œâ”€â”€ ARCHITECTURE.md
â”‚       â””â”€â”€ MIGRATION_GUIDE.md
â”‚
â”œâ”€â”€ SCRAPER_ARCHITECTURE_ANALYSIS.md    # Analysis of old system (IMPORTANT!)
â”œâ”€â”€ PROJECT_STRUCTURE.md                # This file
â”‚
â”œâ”€â”€ ACTION_PLAN.md                      # Original action plan
â”œâ”€â”€ DELIVERY_COMPLETE.md                # Project completion doc
â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md           # Implementation notes
â”œâ”€â”€ SETUP_GUIDE.md                      # Setup instructions
â”œâ”€â”€ QUICK_REFERENCE.md                  # Quick reference
â”œâ”€â”€ LOCAL_TESTING_GUIDE.md              # Testing guide
â””â”€â”€ .git/                               # Git repository

```

---

## What's Where

### ğŸ“– Understanding the System

**START HERE:**
1. Read `SCRAPER_ARCHITECTURE_ANALYSIS.md` - Complete overview of how the old system works
2. Read `scraper_2.2/README.md` - Getting started with new implementation

### ğŸ“š Reference Materials

**Old System (for learning):**
- `old_structure/FINAL_TOOLS_OUTPUT/scraper_core.py` - Scraping logic with Cloudflare bypass
- `old_structure/FINAL_TOOLS_OUTPUT/database.py` - Database patterns and queries
- `old_structure/FINAL_TOOLS_OUTPUT/daily_sync.py` - Job orchestration example
- `old_structure/FINAL_TOOLS_OUTPUT/realtime_fetcher.py` - Real-time update pattern

**GitHub Actions (for reference):**
- `.github/workflows/` - Automation patterns

### ğŸš€ New Implementation

**Build here:**
- `scraper_2.2/src/` - Core classes (scraper, database, cache)
- `scraper_2.2/jobs/` - Job scripts (bulk_fetcher, realtime_updater)
- `scraper_2.2/tests/` - Unit tests

---

## Implementation Map

### Phase 1: Core Scraper (scraper_2.2/src/)

```
Task: Implement src/scraper.py
â”œâ”€â”€ Import Selenium + BeautifulSoup
â”œâ”€â”€ Reference: old_structure/FINAL_TOOLS_OUTPUT/scraper_core.py
â”œâ”€â”€ Create ForexFactoryScraper class
â”œâ”€â”€ Methods:
â”‚   â”œâ”€â”€ fetch_date(date)      â†’ Returns list of events
â”‚   â”œâ”€â”€ fetch_today()         â†’ Shortcut for today
â”‚   â””â”€â”€ classify_impact(text) â†’ Categorize event
â””â”€â”€ Test: Can fetch today's events?
```

### Phase 2: Caching Layer (scraper_2.2/src/)

```
Task: Implement src/cache.py
â”œâ”€â”€ Create CacheManager class
â”œâ”€â”€ Methods:
â”‚   â”œâ”€â”€ set(key, data)        â†’ Store to file/DB
â”‚   â”œâ”€â”€ get(key)              â†’ Retrieve from file/DB
â”‚   â”œâ”€â”€ invalidate()          â†’ Clear cache
â”‚   â””â”€â”€ is_valid(key)         â†’ Check expiration
â””â”€â”€ Test: Can cache and retrieve?
```

### Phase 3: Database (scraper_2.2/src/)

```
Task: Implement src/database.py
â”œâ”€â”€ Reference: old_structure/FINAL_TOOLS_OUTPUT/database.py
â”œâ”€â”€ Create DatabaseManager class
â”œâ”€â”€ Methods:
â”‚   â”œâ”€â”€ insert_events(list)           â†’ New events
â”‚   â”œâ”€â”€ update_actual_values(list)    â†’ Update actuals
â”‚   â”œâ”€â”€ get_events_for_today()        â†’ Query today
â”‚   â””â”€â”€ log_job(name, status)         â†’ Track runs
â””â”€â”€ Test: Can insert/update/query?
```

### Phase 4: Bulk Fetcher Job (scraper_2.2/jobs/)

```
Task: Implement jobs/bulk_fetcher.py
â”œâ”€â”€ Initialize scraper, cache, database
â”œâ”€â”€ Fetch data (today + adjacent days)
â”œâ”€â”€ Cache the results
â”œâ”€â”€ Insert new events to DB
â”œâ”€â”€ Log job completion
â””â”€â”€ Test: Does it populate database?
```

### Phase 5: Real-Time Updater (scraper_2.2/jobs/)

```
Task: Implement jobs/realtime_updater.py
â”œâ”€â”€ Read cached today's events
â”œâ”€â”€ Fetch latest actual values
â”œâ”€â”€ Compare against cache
â”œâ”€â”€ Update DB for changed values
â”œâ”€â”€ Update cache
â””â”€â”€ Test: Does it update actuals?
```

### Phase 6: Job Scheduler (scraper_2.2/jobs/)

```
Task: Implement jobs/scheduler.py
â”œâ”€â”€ Schedule bulk_fetcher every 4 hours
â”œâ”€â”€ Schedule realtime_updater every 5 minutes
â”œâ”€â”€ Handle logging
â”œâ”€â”€ Error handling
â””â”€â”€ Test: Do jobs run automatically?
```

### Phase 7: Tests (scraper_2.2/tests/)

```
Task: Implement unit tests
â”œâ”€â”€ test_scraper.py     â†’ Verify scraping logic
â”œâ”€â”€ test_cache.py       â†’ Verify caching
â”œâ”€â”€ test_database.py    â†’ Verify DB operations
â””â”€â”€ Run: pytest
```

---

## Key Files to Study

### Understanding How Scraping Works

**File:** `old_structure/FINAL_TOOLS_OUTPUT/scraper_core.py`

**Key Methods to Understand:**
- `get_driver()` - Creates Chrome instance with Cloudflare bypass
- `scrape_date()` - Single date scraping with HTML parsing
- `scrape_date_range()` - Multiple dates with rate limiting

**HTML Structure:**
```html
<tr class="calendar__row">
  <td>Date</td>
  <td>Time</td>
  <td>Currency</td>
  <td>Impact</td>
  <td>Event Title</td>
  <td>Actual</td>
  <td>Forecast</td>
  <td>Previous</td>
</tr>
```

### Understanding Database Operations

**File:** `old_structure/FINAL_TOOLS_OUTPUT/database.py`

**Key Methods:**
- `insert_events()` - Insert with deduplication (composite key)
- `update_actual_values()` - Update only when value changes
- `get_events_by_date_range()` - Query events

**Database Schema:**
```sql
Economic_Calendar_FF (
  date,
  time,
  currency,
  impact,
  event,
  actual,
  forecast,
  previous,
  created_at,
  updated_at
)
```

### Understanding Job Orchestration

**Files:**
- `old_structure/FINAL_TOOLS_OUTPUT/daily_sync.py` - Shows how to coordinate scraping + DB operations
- `old_structure/FINAL_TOOLS_OUTPUT/realtime_fetcher.py` - Shows real-time update pattern

**Pattern:**
1. Initialize scraper + database
2. Fetch data
3. Process/classify
4. Insert or update DB
5. Log results

---

## Configuration Reference

### Environment Variables (.env)

```
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=forexfactory
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_password
GITHUB_RUN_ID=local
```

### Configuration File (config.yaml)

```yaml
scraper:
  base_url: "https://www.forexfactory.com/calendar"
  request_delay: 2
  browser_timeout: 30
  cloudflare_wait: 5

cache:
  enabled: true
  ttl_hours: 4
  storage: "json"
  path: "./cache"

database:
  pool_size: 5

jobs:
  bulk_fetch_interval_hours: 4
  realtime_update_interval_minutes: 5
  log_level: "INFO"
```

---

## Development Workflow

### 1. Start with Old System Understanding

```bash
# Read the architecture analysis
cat SCRAPER_ARCHITECTURE_ANALYSIS.md

# Study old scraper code
cat old_structure/FINAL_TOOLS_OUTPUT/scraper_core.py
cat old_structure/FINAL_TOOLS_OUTPUT/database.py
```

### 2. Set Up New Environment

```bash
cd scraper_2.2/
cp .env.example .env
cp config.yaml.example config.yaml
# Edit .env and config.yaml with your settings
```

### 3. Implement Core Components

```bash
# Create src/scraper.py (referencing old scraper_core.py)
# Create src/database.py (referencing old database.py)
# Create src/cache.py (new component)
```

### 4. Implement Jobs

```bash
# Create jobs/bulk_fetcher.py
# Create jobs/realtime_updater.py
# Create jobs/scheduler.py
```

### 5. Test & Verify

```bash
# Test scraper
python -m pytest tests/test_scraper.py

# Test bulk fetch
python jobs/bulk_fetcher.py

# Test real-time update
python jobs/realtime_updater.py
```

### 6. Integrate with GitHub Actions

```bash
# Update .github/workflows/ if needed
# Deploy to production
```

---

## Quick Reference

### Folder Purposes

| Folder | Purpose | Access |
|--------|---------|--------|
| `old_structure/` | Reference implementation | READ ONLY |
| `scraper_2.2/` | New implementation | WRITE HERE |
| `.github/` | Automation workflows | REFERENCE |
| `logs/` | Application logs | WRITE |

### File Sizes (old_structure)

- `scraper_core.py` - 308 lines (core scraping)
- `database.py` - 321 lines (DB operations)
- `daily_sync.py` - 176 lines (job orchestration)
- `realtime_fetcher.py` - 171 lines (real-time job)

### Key Concepts

| Concept | Old Location | New Location |
|---------|--------------|--------------|
| Scraping | `scraper_core.py` | `scraper_2.2/src/scraper.py` |
| Database | `database.py` | `scraper_2.2/src/database.py` |
| Caching | None | `scraper_2.2/src/cache.py` |
| Bulk Job | `monthly_updater.py` | `scraper_2.2/jobs/bulk_fetcher.py` |
| Real-time Job | `realtime_fetcher.py` | `scraper_2.2/jobs/realtime_updater.py` |

---

## Important Notes

### âš ï¸ Don't Delete Old Structure
The `old_structure/` folder is your **reference**. Keep it to:
- Understand how things work
- Copy proven patterns
- Debug issues

### âœ… Start Fresh in scraper_2.2
Build new implementations from scratch:
- Learn from old code
- Improve designs
- Implement your optimization (cache + bulk fetch)

### ğŸ“ Keep Documentation Updated
As you implement:
- Update `scraper_2.2/README.md`
- Create `scraper_2.2/docs/ARCHITECTURE.md`
- Document design decisions

---

## Next Steps

1. **Read** `SCRAPER_ARCHITECTURE_ANALYSIS.md`
2. **Study** `old_structure/FINAL_TOOLS_OUTPUT/` code
3. **Set up** `scraper_2.2/` environment
4. **Implement** Phase 1 (Core Scraper)
5. **Test** with `pytest`
6. **Continue** through remaining phases

---

**Created:** 2025-11-08
**Status:** Initial structure ready - Ready for development
